{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e02b66c-a2c4-4517-9501-c267609dc670",
   "metadata": {},
   "source": [
    "# I Trained an AI Model to Generate Donald Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16351139-456e-44b4-8fa1-fd212aada19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data/training Parameters\n",
    "train_frac = 0.75\n",
    "batch = 200\n",
    "shuffle = 200\n",
    "epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "embedding_units = 64\n",
    "lstm_units = 256\n",
    "dense_units = 256\n",
    "dropout_rate = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95731d-d386-47d3-903e-9d021c476542",
   "metadata": {},
   "source": [
    "Before we train, let's check the devices available on our system. If we don't see any GPU's or other hardware accelerators, our training will run on the CPU (which could be a problem for home machines that cannot throttle the number of available CPU cores for training and the training will exhaust the CPU's resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d66a503-a37d-4f04-b0c2-cf6be13e888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicalDevice(name='/device:CPU:0', device_type='CPU')\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "print(*tf.config.list_logical_devices(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16328a5-0587-4b47-93b0-983c04305adf",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "With AI, data processing is half the battle. So we'll spend a lot of time exploring and processing the data before we build our AI model. I'm going to take the tweets for the year 2020 (with the juiciest takes), and we're only concerned with the text, since we're just trying to make funny tweets. I also want these to run mainly on the CPU, so we have access to our main memory and frankly it's faster for this step (on my machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71558702-dc8f-4fe5-b2de-b544c7f0f4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1952     Biden wants to LOCKDOWN our Country, maybe for...\n",
       "3173     Will be doing a press conference today at 5:00...\n",
       "3343     RT @TomFitton: ICYMI, because of dishonest lib...\n",
       "2026     In my opinion, these patriots did nothing wron...\n",
       "580      RT @KLoeffler: Court packing. Defunding police...\n",
       "                               ...                        \n",
       "7719     RT @LATAMforTRUMP: Our sincere condolences to ...\n",
       "11433                              https://t.co/vVSkTSlM1X\n",
       "7663     RT @MazurikL: Make an entrance. Make a run. TH...\n",
       "12147    RT @DineshDSouza: BIG ANNOUNCEMENT: My new fil...\n",
       "670                                https://t.co/bjC5XWlfOJ\n",
       "Name: text, Length: 12234, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/dtweets.csv')\n",
    "df = df.loc[(df['date'] > '2020-01-01') & (df['date'] < '2020-12-31')]\n",
    "tweets = df['text']\n",
    "tweets = tweets.sample(frac=1)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c023a89-e293-49bf-a0ef-d34dec8a7364",
   "metadata": {},
   "source": [
    "The next thing to do is the encoding. We first split the string into sequences of characters. We can use tensorflow's `unicode_split` function for this. We then encode these characters into a string of integers using tensorflow's `StringLookup` layer. This will also give us insight into the size of our input and output spaces (our vocab size) which will determine our input and output layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595d43ce-0afa-4ce3-bfd6-59a074b980d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 389\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    tweet_chars = tf.strings.unicode_split(tweets, input_encoding='UTF-8')\n",
    "    encode_chars = tf.keras.layers.StringLookup()\n",
    "    encode_chars.adapt(tweet_chars)\n",
    "    vocab_size = encode_chars.vocabulary_size()\n",
    "    print('Vocab Size:', vocab_size)\n",
    "    tweet_char_ids = encode_chars(tweet_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995818d-e41e-4358-b067-16ca0feee310",
   "metadata": {},
   "source": [
    "The next part is going to be lengthy and also shamelessly copied from Tensorflow's NLP Zero to Hero course on YouTube. We're going to record every subsequence of each tweet sequence (from the start up to a set character) into a larger dataset. We're also going to pad each sequence with leading zeros so that each sequence is the same length (the length of a tweet, or 280 characters). Since this cell does take a long time, I've used tqdm to indicate the progress as it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04941640-79c8-447e-98ca-f07163f89074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6d2ced603c407ca96b124100adaa8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating padded n-gram sequences:   0%|          | 0/12234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1614323, 280) int32\n",
      "[[ 0  0  0 ...  0 42  7]\n",
      " [ 0  0  0 ... 42  7 12]\n",
      " [ 0  0  0 ...  7 12  2]\n",
      " ...\n",
      " [ 0  0  0 ... 43 11 22]\n",
      " [ 0  0  0 ... 11 22 40]\n",
      " [ 0  0  0 ... 22 40 49]]\n"
     ]
    }
   ],
   "source": [
    "tweet_seqs = []\n",
    "with tf.device('/device:CPU:0'):\n",
    "    for tweet in tqdm(tweet_char_ids, desc='Creating padded n-gram sequences', total=tweet_char_ids.shape[0]):\n",
    "        subseqs = [tweet[:i+1] for i in range(1, len(tweet))]\n",
    "        subseqs = tf.keras.utils.pad_sequences(subseqs, maxlen=280, padding='pre', truncating='pre', value=0)\n",
    "        tweet_seqs.extend(subseqs)\n",
    "tweet_seqs = np.array(tweet_seqs)\n",
    "print(tweet_seqs.shape, tweet_seqs.dtype)\n",
    "print(tweet_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87619005-3056-4958-9acc-ee3c4f638db5",
   "metadata": {},
   "source": [
    "Now the final preparations. We will create a tensorflow Dataset. The last column of our sequence is the output we're trying to predict. The remaining columns are the input sequences. These will be shuffled and split into batches within our dataset. We'll then split the dataset into training and testing data. This will conclude our data preparation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bdc01f0-4b71-4e4f-ba84-cc40d206914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 279), dtype=tf.int32, name=None), TensorSpec(shape=(None, 389), dtype=tf.float32, name=None))>\n",
      "Input sequences: [[ 0  0  0 ...  6  1 33]\n",
      " [ 0  0  0 ...  3  4  1]\n",
      " [ 0  0  0 ...  1  7  9]\n",
      " ...\n",
      " [ 0  0  0 ... 38 20 41]\n",
      " [ 0  0  0 ...  1 12  7]\n",
      " [ 0  0  0 ... 24  5 11]]\n",
      "Output labels: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "outputs = tweet_seqs[:,-1]\n",
    "sequences = tweet_seqs[:,:-1]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, outputs))\n",
    "dataset = dataset.map(lambda seq, out: (seq, tf.one_hot(out, depth=vocab_size)))\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(shuffle)\n",
    "dataset = dataset.batch(batch)\n",
    "\n",
    "# Display data sample\n",
    "print(dataset)\n",
    "for sequence_batch, output_batch in dataset.take(1):\n",
    "    print(f'Input sequences: {sequence_batch}')\n",
    "    print(f'Output labels: {output_batch}')\n",
    "\n",
    "# Split into training and testing\n",
    "train_num = int(train_frac*len(dataset))\n",
    "train_dataset = dataset.take(train_num)\n",
    "test_dataset = dataset.skip(train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79769664-42c5-4fa9-98e2-e95472bdee60",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Now for the fun part. We'll create an LSTM model with an embedding layer for our input sequences. The output width will be our vocab size. We will also put dropout in between our layers to reduce overfitting in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b59af7-956a-43fc-9721-771f38896043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          24896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 64)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               328704    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 389)               99973     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 519,365\n",
      "Trainable params: 519,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Create model\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, embedding_units),\n",
    "    Dropout(dropout_rate),\n",
    "    LSTM(lstm_units),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(dense_units, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3bf8e-5ce7-4bdc-b77c-7c15533b0699",
   "metadata": {},
   "source": [
    "Now, fingers crossed, we can train this model and not run into issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0ffee-289f-4cb9-ada8-379ae5d5b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
