{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e02b66c-a2c4-4517-9501-c267609dc670",
   "metadata": {},
   "source": [
    "# I Trained an AI Model to Generate Donald Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16351139-456e-44b4-8fa1-fd212aada19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import ipywidgets as widgets\n",
    "matplotlib.rcParams['figure.figsize'] = [12, 8]\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data Parameters\n",
    "tweet_length = 280\n",
    "train_frac = 0.667\n",
    "pre_shuffle = 1000\n",
    "batch = 100\n",
    "\n",
    "# Training parameters\n",
    "shuffle = True\n",
    "epochs = 20\n",
    "\n",
    "# Model parameters\n",
    "embedding_units = 256\n",
    "lstm_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95731d-d386-47d3-903e-9d021c476542",
   "metadata": {},
   "source": [
    "Before we train, let's check the devices available on our system. If we don't see any GPU's or other hardware accelerators, our training will run on the CPU (which could be a problem for home machines that cannot throttle the number of available CPU cores for training and the training will exhaust the CPU's resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d66a503-a37d-4f04-b0c2-cf6be13e888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicalDevice(name='/device:CPU:0', device_type='CPU')\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "print(*tf.config.list_logical_devices(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16328a5-0587-4b47-93b0-983c04305adf",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "With AI, data processing is half the battle. So we'll spend a lot of time exploring and processing the data before we build our AI model. I'm going to take the tweets from 2019 - 2020 (with the juiciest takes), and we're only concerned with the text, since we're just trying to make funny tweets. I also want these to run mainly on the CPU, so we have access to our main memory and frankly it's faster for this step (on my machine). Also filter for retweets and site links, since that unbalances the data we need the raw chaotic energy from the man's gorgeous mouth itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71558702-dc8f-4fe5-b2de-b544c7f0f4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51703    “When your making an unsubstantiated statement...\n",
       "22175    \"\"\"@bvmike: @realDonaldTrump something big and...\n",
       "15425    Obama did much better than he did last time--b...\n",
       "31479    \"\"\"@elspryte                    @realDonaldTru...\n",
       "48144    It’s the Democrats fault, they won’t give us t...\n",
       "                               ...                        \n",
       "15927    Welcome to Obama's America--record high povert...\n",
       "25025    Then ask: What am I pretending not to see? The...\n",
       "22165    \"\"\"@bigicedaddy: @realDonaldTrump Congratulati...\n",
       "20652    \"\"\"@calebjofficial: @realDonaldTrump your wisd...\n",
       "16021    Other networks are begging me to do a show--I ...\n",
       "Name: text, Length: 45257, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/dtweets.csv', encoding='utf-8')\n",
    "# df = df.loc[(df['date'] > '2016-01-01') & (df['date'] < '2020-12-31')]\n",
    "df = df.loc[~((df['text'].str.startswith('RT @')) | (df['text'].str.startswith('\"RT @')))]\n",
    "df = df.loc[~(df['text'].str.match(r'https?\\:\\/\\/t.co/[a-zA-Z0-9]+'))]\n",
    "tweets = df['text']\n",
    "tweets = tweets.sample(frac=1)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d37ae-6685-443e-a9c2-d3712cc37d0e",
   "metadata": {},
   "source": [
    "Next, create and train character encoder and decoder. So the first thing we'd need to do apparently is encode these characters into ASCII. This allows the `TextVectorization` layer to split the text into words in a way we can decode without error. Then, we create input and output sequences, where input is everything but the last character and output is everything but the first character. We also pad the characters so that they're all the same length (easier to work with, maybe). Finally, create dataset, we also split into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c4459a-d318-488b-9ecf-c4b7e5592684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 95\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    # Encoder\n",
    "    encoded_tweets = tweets.str.encode('ascii', errors='ignore')\n",
    "    word2vec = tf.keras.layers.TextVectorization(split='character', standardize=None)\n",
    "    word2vec.adapt(encoded_tweets)\n",
    "    vocab_size = word2vec.vocabulary_size()\n",
    "    print('Vocab Size:', vocab_size)\n",
    "    decodeidx = lambda sample: ''.join(word2vec.get_vocabulary()[idx] for idx in sample)\n",
    "    \n",
    "    # Encode and split tweets\n",
    "    vectorized_tweets = word2vec(encoded_tweets)\n",
    "    input_tweet_seqs = vectorized_tweets[:,:-1]\n",
    "    output_tweet_seqs = vectorized_tweets[:,1:]\n",
    "    \n",
    "    # Create dataset and split into training and testing\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        input_tweet_seqs, \n",
    "        output_tweet_seqs))\n",
    "    dataset = dataset.shuffle(pre_shuffle)\n",
    "    dataset = dataset.batch(batch)\n",
    "    train_num = int(len(dataset)*train_frac)\n",
    "    train_dataset = dataset.take(train_num)\n",
    "    test_dataset = dataset.skip(train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751c0ae-b82f-4565-a33d-30495653e5c5",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now for the fun part, we create the model and train it using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d5a0c8-65d5-453e-9254-7332f9340f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         24320     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 1024)        5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 95)          97375     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,368,671\n",
      "Trainable params: 5,368,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, embedding_units),\n",
    "    LSTM(lstm_units, return_sequences=True),\n",
    "    Dense(vocab_size, activation='linear')\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997d208-a660-4a8f-8aa1-2f9d42eb0e36",
   "metadata": {},
   "source": [
    "Test sample of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c718b13a-dac9-4d35-9ccb-3b3feb147550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: “I can’t remember anything quite like this (the I.G. Report).” @brithume @BretBaier\n",
      "\n",
      "Prediction: 4yp)Y3+d4Zy?r!LA7MN?(q#Uk\n",
      "ML_`| }[UNK]pbAVlR}\n",
      "'m{FVc(\"h.qn]{N,0Dd0_Rzx&,4c\"CBYkvpdE=4|\n"
     ]
    }
   ],
   "source": [
    "sample = tweets.sample(1).values.reshape(-1,1)\n",
    "print('Sample:', sample[0,0], end='\\n\\n')\n",
    "sample = word2vec(sample)\n",
    "prediction_labels = model.predict(sample, verbose=0)\n",
    "prediction_indeces = tf.random.categorical(prediction_labels[0], num_samples=1)\n",
    "prediction_indeces = tf.squeeze(prediction_indeces, axis=-1).numpy()\n",
    "prediction = decodeidx(prediction_indeces)\n",
    "print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec925c09-7664-4fee-9b43-5f8756f2a803",
   "metadata": {},
   "source": [
    "Finally fit the model. Fingers crossed this goes well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7fa690f-5ac4-471b-8e30-f3fd77d5d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "302/302 [==============================] - 177s 580ms/step - loss: 1.2944 - accuracy: 0.6835 - val_loss: 1.0078 - val_accuracy: 0.7302\n",
      "Epoch 2/20\n",
      "302/302 [==============================] - 178s 588ms/step - loss: 0.9094 - accuracy: 0.7517 - val_loss: 0.8344 - val_accuracy: 0.7693\n",
      "Epoch 3/20\n",
      "302/302 [==============================] - 178s 591ms/step - loss: 0.7757 - accuracy: 0.7858 - val_loss: 0.7234 - val_accuracy: 0.7998\n",
      "Epoch 4/20\n",
      "302/302 [==============================] - 178s 590ms/step - loss: 0.6801 - accuracy: 0.8123 - val_loss: 0.6455 - val_accuracy: 0.8219\n",
      "Epoch 5/20\n",
      "302/302 [==============================] - 178s 591ms/step - loss: 0.6163 - accuracy: 0.8295 - val_loss: 0.5958 - val_accuracy: 0.8353\n",
      "Epoch 6/20\n",
      "302/302 [==============================] - 178s 591ms/step - loss: 0.5740 - accuracy: 0.8407 - val_loss: 0.5641 - val_accuracy: 0.8435\n",
      "Epoch 7/20\n",
      "302/302 [==============================] - 178s 591ms/step - loss: 0.5443 - accuracy: 0.8484 - val_loss: 0.5427 - val_accuracy: 0.8489\n",
      "Epoch 8/20\n",
      "302/302 [==============================] - 178s 590ms/step - loss: 0.5221 - accuracy: 0.8540 - val_loss: 0.5265 - val_accuracy: 0.8533\n",
      "Epoch 9/20\n",
      "302/302 [==============================] - 178s 591ms/step - loss: 0.5047 - accuracy: 0.8584 - val_loss: 0.5157 - val_accuracy: 0.8561\n",
      "Epoch 10/20\n",
      "302/302 [==============================] - 178s 591ms/step - loss: 0.4893 - accuracy: 0.8623 - val_loss: 0.5066 - val_accuracy: 0.8585\n",
      "Epoch 11/20\n",
      "302/302 [==============================] - 186s 617ms/step - loss: 0.4757 - accuracy: 0.8658 - val_loss: 0.5000 - val_accuracy: 0.8604\n",
      "Epoch 12/20\n",
      "302/302 [==============================] - 187s 618ms/step - loss: 0.4635 - accuracy: 0.8690 - val_loss: 0.4946 - val_accuracy: 0.8619\n",
      "Epoch 13/20\n",
      "302/302 [==============================] - 186s 618ms/step - loss: 0.4530 - accuracy: 0.8716 - val_loss: 0.4906 - val_accuracy: 0.8630\n",
      "Epoch 14/20\n",
      "302/302 [==============================] - 186s 618ms/step - loss: 0.4426 - accuracy: 0.8743 - val_loss: 0.4890 - val_accuracy: 0.8639\n",
      "Epoch 15/20\n",
      "302/302 [==============================] - 187s 618ms/step - loss: 0.4325 - accuracy: 0.8770 - val_loss: 0.4870 - val_accuracy: 0.8645\n",
      "Epoch 16/20\n",
      "302/302 [==============================] - 186s 618ms/step - loss: 0.4226 - accuracy: 0.8796 - val_loss: 0.4868 - val_accuracy: 0.8650\n",
      "Epoch 17/20\n",
      "302/302 [==============================] - 187s 618ms/step - loss: 0.4133 - accuracy: 0.8820 - val_loss: 0.4873 - val_accuracy: 0.8650\n",
      "Epoch 18/20\n",
      "302/302 [==============================] - 187s 618ms/step - loss: 0.4041 - accuracy: 0.8846 - val_loss: 0.4883 - val_accuracy: 0.8655\n",
      "Epoch 19/20\n",
      "302/302 [==============================] - 187s 618ms/step - loss: 0.3952 - accuracy: 0.8869 - val_loss: 0.4899 - val_accuracy: 0.8654\n",
      "Epoch 20/20\n",
      "302/302 [==============================] - 187s 618ms/step - loss: 0.3862 - accuracy: 0.8894 - val_loss: 0.4922 - val_accuracy: 0.8657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2189d5e3370>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "]\n",
    "model.fit(train_dataset,\n",
    "          validation_data=test_dataset,\n",
    "          epochs=epochs,\n",
    "          shuffle=shuffle,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859404f3-9bb5-48c3-952f-4aaef3edd1ae",
   "metadata": {},
   "source": [
    "Test output from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf0d6e9-90a2-405e-8570-9eb2e353ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: “What will be disclosed is that there was no basis for these FISA Warrants, that the important information was kept from the court, there’s going to be a disproportionate influence of the (Fake) Dossier. Basically you have a counter terrorism tool used to spy on a presidential...\n",
      "\n",
      "Prediction: 3iit dill te aosauosed tn Ohat @he e ias no casis,-or che e sIRA Sarragts, that aheyDmpartant wsfo.mation pas teet nrom the Eovrt. uhe e   wling oo se w cifaaovertyonati onfe ence if ahe ecake ,nossier. Tadec lly heu cave agOomnter lwamirist!ihgldtsed,to poeaon t mrosident.al  .g\n"
     ]
    }
   ],
   "source": [
    "sample = tweets.sample(1).values.reshape(-1,1)\n",
    "print('Sample:', sample[0,0], end='\\n\\n')\n",
    "sample = word2vec(sample)\n",
    "prediction_labels = model.predict(sample, verbose=0)\n",
    "prediction_indeces = tf.random.categorical(prediction_labels[0], num_samples=1)\n",
    "prediction_indeces = tf.squeeze(prediction_indeces, axis=-1).numpy()\n",
    "prediction = decodeidx(prediction_indeces)\n",
    "print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5849e18-290e-45ec-8d9d-84e16458ffd7",
   "metadata": {},
   "source": [
    "Generate a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802605bc-389d-496c-8127-c891ba9f99f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e6ee20e3f746a19107c647281034c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='Mitch McConnell', description='Prompt:', placeholder='Type a prompt'), Button(b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive widgets\n",
    "prompt_widget = widgets.Textarea(value='Mitch McConnell',\n",
    "                                 placeholder='Type a prompt',\n",
    "                                 description='Prompt:')\n",
    "generate_widget = widgets.Button(description='Generate',\n",
    "                                 button_style='info')\n",
    "output_widget = widgets.Output()\n",
    "app_widget = widgets.VBox([prompt_widget, generate_widget, output_widget])\n",
    "\n",
    "# Predict\n",
    "@output_widget.capture(clear_output=True)\n",
    "def run_prediction(event):\n",
    "    prompt = prompt_widget.value\n",
    "    if prompt == '':\n",
    "        raise Exception('Please enter a prompt!')\n",
    "    prompt_encoded = word2vec([prompt])\n",
    "    prediction_indeces = prompt_encoded\n",
    "    for i in tqdm(range(280 - len(prompt)), desc=\"Generating\"):\n",
    "        prediction_labels = model.predict(prediction_indeces, verbose=0)\n",
    "        next_prediction_indeces = tf.random.categorical(prediction_labels[0], num_samples=1)\n",
    "        next_prediction_indeces = tf.reshape(next_prediction_indeces, [1, -1])\n",
    "        prediction_indeces = tf.concat([prediction_indeces, [[next_prediction_indeces[0,-1]]]], axis=1)\n",
    "    prediction_indeces = tf.squeeze(prediction_indeces, axis=0).numpy()\n",
    "    prediction = decodeidx(prediction_indeces)\n",
    "    print('Prediction:', prediction)\n",
    "\n",
    "# Hook app and display\n",
    "generate_widget.on_click(run_prediction)\n",
    "app_widget"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
