{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e02b66c-a2c4-4517-9501-c267609dc670",
   "metadata": {},
   "source": [
    "# I Trained an AI Model to Generate Donald Trump Tweets\n",
    "\n",
    "In this notebook, I use Keras and Tensorflow to train a model on a dataset of tweets made by Donald Trump in order to generate new ones. The current model uses a single embedding layer and a single LSTM layer, with a final validation accuracy of ~80%. Credit to [Brendan Brown](https://github.com/bpb27) who created and hosts the [Trump Twitter Archive](https://www.thetrumparchive.com) which contains the dataset of tweets that I've used to train this model.\n",
    "\n",
    "The first cell below contains the parameters that the notebook uses. Set the `train` parameter to true if you want to train the model. If you just want to play with the model in it's current state, set `train` to false to bypass the training steps and load the model from the notebook directory. After the cell runs, it will list the logical devices available to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16351139-456e-44b4-8fa1-fd212aada19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicalDevice(name='/device:CPU:0', device_type='CPU')\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "from os.path import exists\n",
    "\n",
    "# Data Parameters\n",
    "train_frac = 0.667\n",
    "pre_shuffle = 1000\n",
    "batch = 100\n",
    "\n",
    "# Training parameters\n",
    "train = False\n",
    "shuffle = True\n",
    "epochs = 20\n",
    "\n",
    "# Model parameters\n",
    "embedding_units = 256\n",
    "lstm_units = 1024\n",
    "\n",
    "# Print available devices\n",
    "print(*tf.config.list_logical_devices(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16328a5-0587-4b47-93b0-983c04305adf",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "With AI, data processing is half the battle. So we'll spend a lot of time exploring and processing the data before we build our AI model. We're only concerned with the text, since we're just trying to make funny tweets, so we're gonna filter out tweets that are just links. We'll also filter for retweets (which we can do by removing tweets that begin with \"RT @\"), since we need the raw chaotic energy from the man's gorgeous mouth itself. Of course, we will only run this and the following step if we are training. If not, these two steps are only needed to generate the vocabulary for the vectorizer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71558702-dc8f-4fe5-b2de-b544c7f0f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train or not exists('models/_support/ai-donald-trump-vectorizer.json'):\n",
    "    df = pd.read_csv('data/dtweets.csv', encoding='utf-8')\n",
    "    df = df.loc[~(df['text'].str.match(r'https?\\:\\/\\/t.co/[a-zA-Z0-9]+'))]\n",
    "    df = df.loc[~((df['text'].str.startswith('RT @')) | (df['text'].str.startswith('\"RT @')))]\n",
    "    tweets = df['text']\n",
    "    tweets = tweets.sample(frac=1)\n",
    "    tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d37ae-6685-443e-a9c2-d3712cc37d0e",
   "metadata": {},
   "source": [
    "Next, create and train character encoder and decoder. I want these to run mainly on the CPU, so we have access to our main memory and frankly it's faster for this step (on my machine). So the first thing we'd need to do is encode these characters into ASCII. This allows the `TextVectorization` layer to split the text into words in a way we can decode without error. Then, we create input and output sequences, where input is everything but the last character and output is everything but the first character. Finally, with the input and output arrays, we'll create dataset, we also split into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c4459a-d318-488b-9ecf-c4b7e5592684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 95\n"
     ]
    }
   ],
   "source": [
    "if train or not exists('models/_support/ai-donald-trump-vectorizer.json'):\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        # Encoder\n",
    "        encoded_tweets = tweets.str.encode('ascii', errors='ignore')\n",
    "        word2vec = tf.keras.layers.TextVectorization(split='character', standardize=None)\n",
    "        word2vec.adapt(encoded_tweets)\n",
    "        vocab_size = word2vec.vocabulary_size()\n",
    "        print('Save vectorizer...')\n",
    "        with open('models/_support/ai-donald-trump-vectorizer.json', 'w+') as f:\n",
    "            json.dump(word2vec.get_vocabulary(), f)\n",
    "\n",
    "        # Encode and split tweets\n",
    "        vectorized_tweets = word2vec(encoded_tweets)\n",
    "        input_tweet_seqs = vectorized_tweets[:,:-1]\n",
    "        output_tweet_seqs = vectorized_tweets[:,1:]\n",
    "\n",
    "        # Create dataset and split into training and testing\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            input_tweet_seqs, \n",
    "            output_tweet_seqs))\n",
    "        dataset = dataset.shuffle(pre_shuffle)\n",
    "        dataset = dataset.batch(batch)\n",
    "        train_num = int(len(dataset)*train_frac)\n",
    "        train_dataset = dataset.take(train_num)\n",
    "        test_dataset = dataset.skip(train_num)\n",
    "else:\n",
    "    with open('models/_support/ai-donald-trump-vectorizer.json', 'r') as f:\n",
    "        word2vec = tf.keras.layers.TextVectorization(split='character', \n",
    "                                                     standardize=None, \n",
    "                                                     vocabulary=json.load(f))\n",
    "        vocab_size = word2vec.vocabulary_size()\n",
    "\n",
    "# Make Decoder function and print vocab size\n",
    "decodeidx = lambda sample: ''.join(word2vec.get_vocabulary()[idx] for idx in sample)\n",
    "print('Vocab Size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751c0ae-b82f-4565-a33d-30495653e5c5",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now for the fun part, we create the model and train it using keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d5a0c8-65d5-453e-9254-7332f9340f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         24320     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 1024)        5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 95)          97375     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,368,671\n",
      "Trainable params: 5,368,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# TODO: Create model using keras object oriented\n",
    "#       framework and be able to pass state to \n",
    "#       LSTM if needed\n",
    "\n",
    "# Load or create model\n",
    "if train:\n",
    "    print('Creating model...')\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(vocab_size, embedding_units),\n",
    "        LSTM(lstm_units, return_sequences=True),\n",
    "        Dense(vocab_size, activation='linear')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "else:\n",
    "    print('Loading model...')\n",
    "    model = tf.keras.models.load_model('models/ai-donald-trump')\n",
    "    \n",
    "# Print summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec925c09-7664-4fee-9b43-5f8756f2a803",
   "metadata": {},
   "source": [
    "In this cell we train the model with the data. Given the training parameters and our hardware performance, this will take a while. Make sure this cell runs until completion. Fingers crossed this goes well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fa690f-5ac4-471b-8e30-f3fd77d5d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    model.fit(train_dataset,\n",
    "              validation_data=test_dataset,\n",
    "              epochs=epochs,\n",
    "              shuffle=shuffle,\n",
    "              callbacks=[\n",
    "                  tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "              ])\n",
    "    model.save('models/ai-donald-trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5849e18-290e-45ec-8d9d-84e16458ffd7",
   "metadata": {},
   "source": [
    "Now that our model has successfully trained, we'll generate a sentence using it. I have a widget below that allows you to input a prompt and see what the model spits out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802605bc-389d-496c-8127-c891ba9f99f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040e4f395022493f8bdf16c1d6286a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='Mitch McConnell', description='Prompt:', placeholder='Type a prompt'), Button(b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Implement OneStepGenerator model\n",
    "\n",
    "# Interactive widgets\n",
    "prompt_widget = widgets.Textarea(value='Mitch McConnell',\n",
    "                                 placeholder='Type a prompt',\n",
    "                                 description='Prompt:')\n",
    "generate_widget = widgets.Button(description='Generate',\n",
    "                                 button_style='info')\n",
    "output_widget = widgets.Output()\n",
    "app_widget = widgets.VBox([\n",
    "    prompt_widget, \n",
    "    generate_widget, \n",
    "    output_widget\n",
    "])\n",
    "\n",
    "\n",
    "# Prediction routine\n",
    "@output_widget.capture(clear_output=True)\n",
    "def run_prediction(event):\n",
    "    \"\"\"\n",
    "    Generate a sentence with prompt \n",
    "    provided by widget\n",
    "    \n",
    "    :param event: button click event\n",
    "    \"\"\"\n",
    "    # Get prompt value\n",
    "    prompt = prompt_widget.value\n",
    "    if prompt == '':\n",
    "        raise Exception('Please enter a prompt!')\n",
    "    \n",
    "    # Encode prompt\n",
    "    prompt_encoded = word2vec([prompt])\n",
    "    \n",
    "    # Generate tweet chars starting from prompt\n",
    "    tweet_length = 280\n",
    "    prediction_indeces = prompt_encoded\n",
    "    for i in tqdm(range(tweet_length - len(prompt)), desc=\"Generating\"):\n",
    "        prediction_labels = model.predict(prediction_indeces, verbose=0)\n",
    "        next_prediction_indeces = tf.random.categorical(prediction_labels[0], num_samples=1)\n",
    "        next_prediction_indeces = tf.reshape(next_prediction_indeces, [1, -1])\n",
    "        prediction_indeces = tf.concat([prediction_indeces, [[next_prediction_indeces[0,-1]]]], axis=1)\n",
    "    prediction_indeces = tf.squeeze(prediction_indeces, axis=0).numpy()\n",
    "    \n",
    "    # Decode and print prediction\n",
    "    prediction = decodeidx(prediction_indeces)\n",
    "    print('Prediction:', prediction)\n",
    "\n",
    "\n",
    "# Hook up app and display\n",
    "generate_widget.on_click(run_prediction)\n",
    "app_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d136e6-ff95-40c9-b69d-3eaa2896432e",
   "metadata": {},
   "source": [
    "It took a long while to get this model working. I've actually made a separate repository with code to train this model, but training in a notebook seems to be the preferred way to do it (I might be wrong). Regardless, it is quite incredible to actually see it working! I have thought of improvements I could make with this model, the least of which actually would be training for more epochs. I feel it would be more beneficial to better clean and filter out the data, while also experimenting with different model structures and training parameters. The latter can actually be done programatically using hyperparameter tuning, which I've done in the past (in the aformentioned repo). The data, however, can be sifted through a bit better. I've noticed that the generated tweets often end in generated hyperlinks (which will lead to nowhere, though it might be concerning if the model generated an actual link). So, filtering out tweets that end in hyperlinks (or filtering out hyperlinks entirely) may lead to better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
