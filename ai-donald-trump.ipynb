{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e02b66c-a2c4-4517-9501-c267609dc670",
   "metadata": {},
   "source": [
    "# I Trained an AI Model to Generate Donald Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16351139-456e-44b4-8fa1-fd212aada19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data Parameters\n",
    "tweet_length = 280\n",
    "train_frac = 0.75\n",
    "batch = 200\n",
    "\n",
    "# Training parameters\n",
    "shuffle = True\n",
    "epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "embedding_units = 64\n",
    "lstm_1_units = 256\n",
    "lstm_2_units = 256\n",
    "dense_units = 256\n",
    "dropout_rate = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95731d-d386-47d3-903e-9d021c476542",
   "metadata": {},
   "source": [
    "Before we train, let's check the devices available on our system. If we don't see any GPU's or other hardware accelerators, our training will run on the CPU (which could be a problem for home machines that cannot throttle the number of available CPU cores for training and the training will exhaust the CPU's resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d66a503-a37d-4f04-b0c2-cf6be13e888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicalDevice(name='/device:CPU:0', device_type='CPU')\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "print(*tf.config.list_logical_devices(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16328a5-0587-4b47-93b0-983c04305adf",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "With AI, data processing is half the battle. So we'll spend a lot of time exploring and processing the data before we build our AI model. I'm going to take the tweets for the year 2020 (with the juiciest takes), and we're only concerned with the text, since we're just trying to make funny tweets. I also want these to run mainly on the CPU, so we have access to our main memory and frankly it's faster for this step (on my machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71558702-dc8f-4fe5-b2de-b544c7f0f4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11052    .@RepKevinBrady (R) of Texas-08 loves Texas &a...\n",
       "4841     RT @GOPChairwoman: “As one grateful nation, we...\n",
       "7418     Disgraceful Anarchists. We are watching them c...\n",
       "11352    RT @charliekirk11: All the lockdowns must end ...\n",
       "6375     RT @TrumpWarRoom: HISTORIC: After 49 years, Is...\n",
       "                               ...                        \n",
       "1982     Under my leadership, our ECONOMY is now growin...\n",
       "9802     RT @charliekirk11: One week ago today, Democra...\n",
       "11867    RT @PeteHegseth: This Atlantic “story” is noth...\n",
       "5638     Many Democrats want to Defund and Abolish Poli...\n",
       "631      Look at this in Wisconsin! A day AFTER the ele...\n",
       "Name: text, Length: 12234, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/dtweets.csv')\n",
    "df = df.loc[(df['date'] > '2020-01-01') & (df['date'] < '2020-12-31')]\n",
    "tweets = df['text']\n",
    "tweets = tweets.sample(frac=1)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d37ae-6685-443e-a9c2-d3712cc37d0e",
   "metadata": {},
   "source": [
    "Next, split characters and train character encoder and decoder. Create input and output sequences, where input is everything but the last character and output is everything but the first character. We also pad the characters so that they're all the same length (easier to work with, maybe). Finally, create dataset, we also split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7c4459a-d318-488b-9ecf-c4b7e5592684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 389\n",
      "Dataset: DatasetSpec((TensorSpec(shape=(None, 279), dtype=tf.int32, name=None), TensorSpec(shape=(None, 279, 389), dtype=tf.float32, name=None)), TensorShape([]))\n",
      "Top Start Chars: [b'R' b'T' b'h' b'.' b'I' b'W' b'S' b'C' b'G']\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "    # Encode chars\n",
    "    tweet_chars = tf.strings.unicode_split(tweets, input_encoding='UTF-8')\n",
    "    encode_chars = tf.keras.layers.StringLookup()\n",
    "    encode_chars.adapt(tweet_chars)\n",
    "    vocab_size = encode_chars.vocabulary_size()\n",
    "    print('Vocab Size:', vocab_size)\n",
    "    decode_chars = tf.keras.layers.StringLookup(invert=True, \n",
    "                                                vocabulary=encode_chars.get_vocabulary())\n",
    "    \n",
    "    # Create padded input and output sequences\n",
    "    tweet_charids = encode_chars(tweet_chars).to_list()\n",
    "    input_tweet_charids = [ list(tensor)[:-1] for tensor in tweet_charids ]\n",
    "    output_tweet_charids = [ list(tensor)[1:] for tensor in tweet_charids ]\n",
    "    input_sequences = tf.keras.utils.pad_sequences(input_tweet_charids, \n",
    "                                                   maxlen=(tweet_length - 1), \n",
    "                                                   padding='pre', \n",
    "                                                   truncating='pre')\n",
    "    output_sequences = tf.keras.utils.pad_sequences(output_tweet_charids, \n",
    "                                                    maxlen=(tweet_length - 1), \n",
    "                                                    padding='pre', \n",
    "                                                    truncating='pre')\n",
    "    output_labels = tf.one_hot(output_sequences, depth=vocab_size)\n",
    "    \n",
    "    # Create training and testing dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_sequences, output_labels))\n",
    "    dataset = dataset.batch(batch)\n",
    "    print('Dataset:', tf.data.DatasetSpec.from_value(dataset))\n",
    "    train_num = int(train_frac*len(dataset))\n",
    "    train_dataset = dataset.take(train_num)\n",
    "    test_dataset = dataset.skip(train_num)\n",
    "    \n",
    "    # Get top start characters\n",
    "    start_chars = np.array([ seq[0] for seq in input_tweet_charids ])\n",
    "    uqsc, counts = np.unique(start_chars, return_counts=True)\n",
    "    order = np.flip(np.argsort(counts))[:9]\n",
    "    top_start_chars = uqsc[order]\n",
    "    print('Top Start Chars:', decode_chars(top_start_chars).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751c0ae-b82f-4565-a33d-30495653e5c5",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now for the fun part, we create the model and train it using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d5a0c8-65d5-453e-9254-7332f9340f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          24896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 64)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 256)         328704    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 256)         525312    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 389)         99973     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,044,677\n",
      "Trainable params: 1,044,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, embedding_units),\n",
    "    Dropout(dropout_rate),\n",
    "    LSTM(lstm_1_units, return_sequences=True),\n",
    "    Dropout(dropout_rate),\n",
    "    LSTM(lstm_2_units, return_sequences=True),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(dense_units, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec925c09-7664-4fee-9b43-5f8756f2a803",
   "metadata": {},
   "source": [
    "Finally fit the model. Fingers crossed this goes well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7fa690f-5ac4-471b-8e30-f3fd77d5d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "46/46 [==============================] - 29s 519ms/step - loss: 2.9194 - accuracy: 0.5149 - val_loss: 2.3422 - val_accuracy: 0.5343\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 18s 385ms/step - loss: 2.1197 - accuracy: 0.5394 - val_loss: 1.7034 - val_accuracy: 0.5986\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 18s 392ms/step - loss: 1.6889 - accuracy: 0.5929 - val_loss: 1.6367 - val_accuracy: 0.6020\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 18s 394ms/step - loss: 1.6588 - accuracy: 0.5956 - val_loss: 1.6247 - val_accuracy: 0.6021\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 18s 389ms/step - loss: 1.6482 - accuracy: 0.5958 - val_loss: 1.6147 - val_accuracy: 0.6022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a74eb0d210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset,\n",
    "          validation_data=test_dataset,\n",
    "          epochs=epochs,\n",
    "          shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859404f3-9bb5-48c3-952f-4aaef3edd1ae",
   "metadata": {},
   "source": [
    "Test output from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "362e8b8a-d8d7-4d8d-813d-0f15556d3803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4a55064c6446a2b710621d694ec884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'W                                                                                                                                                                                                                                                                                       ', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "model.reset_states()\n",
    "sequence = np.array([[choice(top_start_chars)]])\n",
    "for i in tqdm(range(tweet_length - 1), desc='Generating'):\n",
    "    labels = model.predict(sequence[0,-1:].reshape(-1,1), verbose=0)\n",
    "    nextch = np.argmax(labels, axis=2)\n",
    "    sequence = np.hstack((sequence, nextch))\n",
    "    \n",
    "sequence = decode_chars(sequence)\n",
    "sequence = tf.strings.reduce_join(sequence)\n",
    "print(sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
