{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5a618a-602b-4af1-a497-53dfaf4aad99",
   "metadata": {},
   "source": [
    "# Object Tracking Using SIFT\n",
    "\n",
    "This notebook is tied to the previous notebook where I recreated the SIFT detector. Now I'll use the SIFT detector to see if I can track objects or patterns within an image. I'll be using the same traffic video that I did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30a8cd4d-c4db-42f1-936b-4358593d2e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Video\n",
    "from matplotlib import pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "video_file = '../../data/traffic-video.mp4'\n",
    "intermediate_file_template = '../../data/traffic-video-{step}.mp4'\n",
    "\n",
    "Video(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553d33f-8ddb-4e12-b008-51b050700596",
   "metadata": {},
   "source": [
    "Let's first see if we can track the grayscale version of the video file. So we'll convert the video to grayscale first. We need to grab the frame size and framerate from our previous capture. Since we'll be writing a lot of video, I'll create some functions to handle most of the common routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a09e3563-e0d1-41dc-9277-75f530be83db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7c62036dd5486c819a2ea25e46b50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Frames:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-grayscale.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@contextmanager\n",
    "def video_pipe(from_file, to_file, fourcc='avc1', isColor=True):\n",
    "    # Create capture and get params\n",
    "    video_capture = cv2.VideoCapture(from_file)\n",
    "    width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    framerate = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Create writer with params\n",
    "    fourcc = cv2.VideoWriter_fourcc(*fourcc)\n",
    "    video_writer = cv2.VideoWriter(to_file, \n",
    "                                   fourcc, framerate, \n",
    "                                   (width, height), \n",
    "                                   isColor=isColor)\n",
    "    \n",
    "    # Yield capture and writer\n",
    "    yield (video_capture, video_writer)\n",
    "    \n",
    "    # Close capture and writer\n",
    "    video_writer.release()\n",
    "    video_capture.release()\n",
    "    \n",
    "def frame_iter(capture, desc='Processing Frames'):\n",
    "    frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    progress = tqdm(desc=desc, total=frames)\n",
    "    has_frames, frame = capture.read()\n",
    "    while has_frames:\n",
    "        yield frame\n",
    "        progress.update()\n",
    "        has_frames, frame = capture.read()\n",
    "    \n",
    "grayscale_file = intermediate_file_template.format(step='grayscale')\n",
    "\n",
    "# Convert frames to grayscale\n",
    "with video_pipe(video_file, grayscale_file, isColor=False) as (cap, writer):\n",
    "    for frame in frame_iter(cap):\n",
    "        grayscale = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        writer.write(grayscale)\n",
    "\n",
    "# Display new Video\n",
    "Video(grayscale_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33639ab7-9d45-4306-acb0-5eb995370f86",
   "metadata": {},
   "source": [
    "Next, we'll apply OpenCV's SIFT detector to each frame. In this example I'm simply going to run the detection and save the frame, so we can play it back and see all of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45ffb1b6-672b-4bc2-943a-e751740113c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfcd205fbd04133bc00e6b211d253a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Frames:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-keypoints.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_features = 0\n",
    "\n",
    "keypoints_file = intermediate_file_template.format(step='keypoints')\n",
    "\n",
    "sift = cv2.SIFT.create(number_of_features)\n",
    "\n",
    "with video_pipe(grayscale_file, keypoints_file) as (cap, wrt):\n",
    "    for frame in frame_iter(cap):\n",
    "        keypoints = sift.detect(frame)\n",
    "        keypoints_frame = cv2.drawKeypoints(frame, keypoints, 0, \n",
    "            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        wrt.write(keypoints_frame)\n",
    "        \n",
    "Video(keypoints_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
