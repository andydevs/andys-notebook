{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5a618a-602b-4af1-a497-53dfaf4aad99",
   "metadata": {},
   "source": [
    "# Object Tracking Using SIFT\n",
    "\n",
    "This notebook is tied to the previous notebook where I recreated the SIFT detector. Now I'll use the SIFT detector to see if I can track objects or patterns within an image. I'll be using the same traffic video that I did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "30a8cd4d-c4db-42f1-936b-4358593d2e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Video\n",
    "from ipywidgets import Output\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import cycle\n",
    "from operator import itemgetter\n",
    "from uuid import UUID, uuid1 as uuid\n",
    "from dataclasses import dataclass, field\n",
    "from collections import OrderedDict\n",
    "\n",
    "matplotlib.rcdefaults()\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = (16, 12)\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "intpl = lambda t: tuple(map(int, t))\n",
    "take = lambda n, it: map(itemgetter(1), zip(range(n), it))\n",
    "\n",
    "video_file = '../../data/traffic-video.mp4'\n",
    "intermediate_file_template = '../../data/traffic-video-{step}.mp4'\n",
    "\n",
    "Video(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553d33f-8ddb-4e12-b008-51b050700596",
   "metadata": {},
   "source": [
    "Let's first see if we can track the grayscale version of the video file. So we'll convert the video to grayscale first. We need to grab the frame size and framerate from our previous capture. Since we'll be writing a lot of video, I'll create some functions to handle most of the common routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a09e3563-e0d1-41dc-9277-75f530be83db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d820708556804d88a8621601e7f4a9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Frames:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-grayscale.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@contextmanager\n",
    "def video_pipe(from_file, to_file, fourcc='avc1', isColor=True):\n",
    "    # Create capture and get params\n",
    "    video_capture = cv2.VideoCapture(from_file)\n",
    "    width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    framerate = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Create writer with params\n",
    "    fourcc = cv2.VideoWriter_fourcc(*fourcc)\n",
    "    video_writer = cv2.VideoWriter(to_file, \n",
    "                                   fourcc, framerate, \n",
    "                                   (width, height), \n",
    "                                   isColor=isColor)\n",
    "    \n",
    "    # Yield capture and writer\n",
    "    yield (video_capture, video_writer)\n",
    "    \n",
    "    # Close capture and writer\n",
    "    video_writer.release()\n",
    "    video_capture.release()\n",
    "    \n",
    "def frame_iter(capture, desc='Processing Frames'):\n",
    "    frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    progress = tqdm(desc=desc, total=frames)\n",
    "    has_frames, frame = capture.read()\n",
    "    while has_frames:\n",
    "        yield frame\n",
    "        progress.update()\n",
    "        has_frames, frame = capture.read()\n",
    "    \n",
    "grayscale_file = intermediate_file_template.format(step='grayscale')\n",
    "\n",
    "# Convert frames to grayscale\n",
    "with video_pipe(video_file, grayscale_file, isColor=False) as (cap, writer):\n",
    "    for frame in frame_iter(cap):\n",
    "        grayscale = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        writer.write(grayscale)\n",
    "\n",
    "# Display new Video\n",
    "Video(grayscale_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33639ab7-9d45-4306-acb0-5eb995370f86",
   "metadata": {},
   "source": [
    "Before we apply OpenCV's SIFT detector to each frame, we need to mask out the parts of the image we want to focus on, i.e. the cars driving on the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6543c52e-5b6c-42a3-b948-0ea3c6881d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ada4af638db4230bebdba8d137a2a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Frames:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-masked.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Road line boundary\n",
    "r_x1 = 860\n",
    "r_x2 = 150\n",
    "\n",
    "masked_file = intermediate_file_template.format(step='masked')\n",
    "\n",
    "# Get width and height of image\n",
    "video_capture = cv2.VideoCapture(grayscale_file)\n",
    "w = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video_capture.release()\n",
    "\n",
    "# Image pixel space\n",
    "x,y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "\n",
    "# Mask\n",
    "road_mask = x + y * (r_x1 - r_x2) / h - r_x1 > 0\n",
    "chrmask = ((-x + 665 > 0) \\\n",
    "        | (-y + 640 > 0) \\\n",
    "        | (x - 1155 > 0) \\\n",
    "        | (y - 690 > 0))\n",
    "track_mask = chrmask & road_mask\n",
    "track_mask = track_mask.astype(np.uint8)\n",
    "\n",
    "# Create framed video\n",
    "with video_pipe(grayscale_file, masked_file, isColor=False) as (cap, writer):\n",
    "    for frame in frame_iter(cap):\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        masked_frame = np.where(track_mask, gray_frame, 0)\n",
    "        writer.write(masked_frame)\n",
    "        \n",
    "# Display new Video\n",
    "Video(masked_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71e907-8425-4952-b6a8-6fe5eb2b8fdf",
   "metadata": {},
   "source": [
    "I'm going to draw the points manually as I want to implement my own tracking but still display them. So I created my own colormap to distinguish the different keypoints. All colormap points are displayed neatly below in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3b1cbac9-1750-407b-9c4e-0de9ec83a257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x3600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create colormap\n",
    "n_colors = 360\n",
    "q = n_colors // 6\n",
    "p = np.linspace(0, 1, q)\n",
    "w = np.zeros(n_colors + 2*q)\n",
    "w[2*q:3*q] = p\n",
    "w[3*q:5*q] = 1.0\n",
    "w[5*q:6*q] = 1.0 - p\n",
    "colormap = 255 * np.array([(1.0 - w[q:-q]), w[:-2*q], w[2*q:]]).T\n",
    "np.random.shuffle(colormap)\n",
    "\n",
    "# Display colormap as image\n",
    "imheight = 10\n",
    "imscale = 1\n",
    "imwidth = n_colors // imheight\n",
    "x, y = np.meshgrid(np.arange(imwidth), np.arange(imheight))\n",
    "image = colormap[(y*imwidth + x),:].astype(np.uint8)\n",
    "rc_params = {\n",
    "    'figure.figsize': (imheight*imscale, imwidth*imscale),\n",
    "    'xtick.bottom': False,\n",
    "    'xtick.labelbottom': False,\n",
    "    'ytick.left': False,\n",
    "    'ytick.labelleft': False\n",
    "}\n",
    "with plt.rc_context(rc_params):\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Convert colormap to list of tuples\n",
    "colormap = list(map(tuple, colormap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8c6b2-aee7-4dec-acb1-8b76a7eaaf10",
   "metadata": {},
   "source": [
    "Now we'll run SIFT detection to find all keypoints and descriptors in the frame.\n",
    "\n",
    "Since I'm working on my own tracking algorithm, I'll use my own keypoint drawing system using the custom colormap above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "45ffb1b6-672b-4bc2-943a-e751740113c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c34bcbcb63a41cfa41b3a2211df0837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting Keypoint Data:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-keypoints.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_file = intermediate_file_template.format(step='keypoints')\n",
    "\n",
    "def draw_keypoints(img, keypoints, colors=cycle(colormap)):\n",
    "    kpimg = img.copy()\n",
    "    for kp, color in zip(keypoints, colors):\n",
    "        r = int(kp.size / 2)\n",
    "        c = np.array(kp.pt, dtype=np.int32)\n",
    "        v = np.array([ np.cos(kp.angle), np.sin(kp.angle) ])\n",
    "        u = (c + r*v).astype(np.uint32)\n",
    "        cv2.circle(kpimg, c, r, color, 1)\n",
    "        cv2.arrowedLine(kpimg, c, u, color, 1)\n",
    "    return kpimg\n",
    "        \n",
    "sift = cv2.SIFT.create()\n",
    "keypoint_data = []\n",
    "\n",
    "with video_pipe(grayscale_file, keypoints_file) as (cap, wrt):\n",
    "    for frame in frame_iter(cap, desc=\"Getting Keypoint Data\"):\n",
    "        keypoints, descriptors = sift.detectAndCompute(frame, track_mask)\n",
    "        keypoint_data.append((keypoints, descriptors))\n",
    "        keypoints_frame = draw_keypoints(frame, keypoints)\n",
    "        wrt.write(keypoints_frame)\n",
    "        \n",
    "Video(keypoints_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc48437-9467-4409-ac99-4550ee0f43c4",
   "metadata": {},
   "source": [
    "If we look at these keypoints, we notice they're jittering a lot. That's because each frame is treated independently, and each detection is treated as a new keypoint. We need to correlate each keypoint in a frame with the keypoints in the frame before using a matching algorithm.\n",
    "\n",
    "We'll use the euclidean distance to match the descriptors of keypoints with each other. For each pair of adjacent frames, we take both sets of descriptors, and determine which keypoints match with the previous frame, and which keypoints are new. We can use a distance treshold value to determine these. If keypoints are more similar i.e. low distance, they're correlated. Likewise, if a keypoint's distance is far enough from all other keypoints in the previous frame, it is considered a new keypoint.\n",
    "\n",
    "We'll keep this data in a dict, with the keys being a set of ID's assigned to each keypoint track, and the value being a list of keypoint datas indicating a track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2cdf7789-318c-4b3a-9973-a795567894ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d576195a12154b06b4295ce449d9244f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Tracks:   0%|          | 0/749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 [False  True] [196 360]\n",
      "556 [False  True] [592   9]\n",
      "601 [False  True] [627  12]\n",
      "639 [False  True] [605   3]\n",
      "608 [False  True] [624   9]\n",
      "633 [False  True] [616  18]\n",
      "634 [False  True] [587  48]\n",
      "635 [False  True] [541  72]\n",
      "613 [False  True] [586  47]\n",
      "633 [False  True] [610  39]\n",
      "649 [False  True] [631  42]\n",
      "673 [False  True] [627  24]\n",
      "651 [False  True] [625  26]\n",
      "651 [False  True] [553 107]\n",
      "660 [False  True] [550  94]\n",
      "644 [False  True] [556  97]\n",
      "653 [False  True] [539 111]\n",
      "650 [False  True] [480 180]\n",
      "660 [False  True] [453 197]\n",
      "650 [False  True] [448 206]\n",
      "654 [False  True] [344 307]\n",
      "651 [False  True] [302 367]\n",
      "669 [False  True] [277 388]\n",
      "665 [False  True] [314 349]\n",
      "663 [False  True] [636  24]\n",
      "660 [False  True] [469 211]\n",
      "680 [False  True] [324 350]\n",
      "674 [False  True] [336 345]\n",
      "681 [False  True] [358 321]\n",
      "679 [False  True] [331 345]\n",
      "676 [False  True] [312 366]\n",
      "678 [False  True] [352 335]\n",
      "687 [False  True] [285 399]\n",
      "684 [False  True] [226 448]\n",
      "674 [False  True] [251 435]\n",
      "686 [False  True] [299 386]\n",
      "685 [False  True] [230 456]\n",
      "686 [False  True] [211 481]\n",
      "692 [False  True] [179 515]\n",
      "694 [False  True] [295 403]\n",
      "698 [False  True] [324 359]\n",
      "683 [False  True] [239 460]\n",
      "699 [False  True] [199 494]\n",
      "693 [False  True] [219 484]\n",
      "703 [False  True] [208 498]\n",
      "706 [False  True] [228 490]\n",
      "718 [False  True] [165 559]\n",
      "724 [False  True] [191 529]\n",
      "720 [False  True] [193 534]\n",
      "727 [False  True] [207 518]\n",
      "20100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda0aba6f74544caa6de8a428b2ed870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Frames:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-tracked.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracked_file = intermediate_file_template.format(step='tracked')\n",
    "\n",
    "track_threshold = 10\n",
    "\n",
    "@dataclass\n",
    "class TrackDatapoint:\n",
    "    descriptor: np.ndarray\n",
    "    keypoint: cv2.KeyPoint\n",
    "    \n",
    "@dataclass\n",
    "class Track:\n",
    "    uuid: UUID = field(default_factory=uuid)\n",
    "    start_frame: int = 0\n",
    "    track: list[TrackDatapoint] = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def end_frame(self):\n",
    "        return self.start_frame + len(self.track) - 1\n",
    "    \n",
    "    @property\n",
    "    def latest(self):\n",
    "        return self.track[-1]\n",
    "    \n",
    "class TracksDB:\n",
    "    def __init__(self):\n",
    "        self.db = OrderedDict()\n",
    "        \n",
    "    def new_track(self, frame_idx, dsc, kp):\n",
    "        dp_obj = TrackDatapoint(descriptor=dsc, keypoint=kp)\n",
    "        track_obj = Track(start_frame=frame_idx,\n",
    "                          track=[dp_obj])\n",
    "        self.db[track_obj.uuid] = track_obj\n",
    "        \n",
    "    def tracks_for_frame(self, frame_idx):\n",
    "        return filter(\n",
    "            lambda t: t.end_frame == frame_idx,\n",
    "            self.db.values()\n",
    "        )\n",
    "    \n",
    "    def add_datapoint(self, tid, dsc, kp):\n",
    "        dp_obj = TrackDatapoint(descriptor=dsc, keypoint=kp)\n",
    "        self.db[tid].track.append(dp_obj)\n",
    "        \n",
    "\n",
    "tracks_db = TracksDB()\n",
    "\n",
    "\n",
    "def match_descriptors(dsc_a, dsc_b):\n",
    "    dsc_a = dsc_a[:,np.newaxis,:]\n",
    "    dsc_b = dsc_b[np.newaxis,:,:]\n",
    "    dst = np.sum((dsc_b - dsc_a)**2, axis=2)\n",
    "    return dst.argmin(axis=1), dst.min(axis=1)\n",
    "\n",
    "kps_0, dsc_0 = keypoint_data[0]\n",
    "for kp, dsc in zip(kps_0, dsc_0):\n",
    "    tracks_db.new_track(0, dsc, kp)\n",
    "    \n",
    "kp_iterator = tqdm(\n",
    "    enumerate(keypoint_data[1:]), \n",
    "    desc='Creating Tracks', \n",
    "    total=len(keypoint_data)-1)\n",
    "    \n",
    "for j, (kps_list, current_dsc) in take(50, kp_iterator):\n",
    "    try:\n",
    "        i = j + 1\n",
    "\n",
    "        # Get last track ids and descriptors for frame\n",
    "        current_kps = np.array(kps_list)\n",
    "        last_track_ids = np.array([ t.uuid for t in tracks_db.tracks_for_frame(j) ])\n",
    "        last_track_dsc = np.array([ t.latest.descriptor for t in tracks_db.tracks_for_frame(j) ])\n",
    "        \n",
    "        # If we have any tracks\n",
    "        if last_track_dsc.shape[0] > 0:\n",
    "            # Match current descriptors with last descriptors and find the \n",
    "            # minimum distance. We then use this to find which descriptors \n",
    "            # correspond to new keypoints and which ones correspond to \n",
    "            # keypoints we've already seen and create masks for each\n",
    "            m_idx, m_dst = match_descriptors(current_dsc, last_track_dsc)\n",
    "            tracked = m_dst < track_threshold\n",
    "            print(last_track_dsc.shape[0], *np.unique(tracked, return_counts=True))\n",
    "            new_track = ~tracked\n",
    "\n",
    "            # For tracked keypoints, we'll associate the new keypoints \n",
    "            # and descriptors with their corresponding ids. Then add \n",
    "            # those new track values to the database\n",
    "            tracked_kps = current_kps[tracked]\n",
    "            tracked_dsc = current_dsc[tracked]\n",
    "            tracked_ids = last_track_ids[m_idx[tracked]]\n",
    "            for tid, dsc, kp in zip(tracked_ids, tracked_dsc, tracked_kps):\n",
    "                tracks_db.add_datapoint(tid, dsc, kp)\n",
    "                \n",
    "            # We'll create new tracks for new keypoints starting \n",
    "            # at current frame\n",
    "            new_kps = current_kps[new_track]\n",
    "            new_dsc = current_dsc[new_track]\n",
    "            for dsc, kp in zip(new_dsc, new_kps):\n",
    "                tracks_db.new_track(i, dsc, kp)\n",
    "        else:\n",
    "            # Add all current keypoints and descriptors as new tracks\n",
    "            for dsc, kp in zip(current_dsc, current_kps):\n",
    "                tracks_db.new_track(i, dsc, kp)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(list(tracks_db.tracks_for_frame(j)))\n",
    "        print(last_track_dsc.shape)\n",
    "        print(current_dsc.shape)\n",
    "        raise e\n",
    "        \n",
    "print(len(tracks_db.db.values()))\n",
    "        \n",
    "# Assign colors to ids\n",
    "colors_for_ids = {\n",
    "    track.uuid: color\n",
    "    for track, color \n",
    "    in zip( tracks_db.db.values(), cycle(colormap) )\n",
    "}\n",
    "\n",
    "with video_pipe(grayscale_file, tracked_file) as (cap, wrt):\n",
    "    for i,frame in take(50, enumerate(frame_iter(cap))):\n",
    "        tracks = list(tracks_db.tracks_for_frame(i))\n",
    "        kps = [ track.latest.keypoint for track in tracks ]\n",
    "        colors = [ colors_for_ids[track.uuid] for track in tracks ]\n",
    "        tracked_frame = draw_keypoints(frame, kps, colors)\n",
    "        wrt.write(tracked_frame)\n",
    "        \n",
    "Video(tracked_file)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
