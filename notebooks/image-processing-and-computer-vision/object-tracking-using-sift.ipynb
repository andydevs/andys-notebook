{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5a618a-602b-4af1-a497-53dfaf4aad99",
   "metadata": {},
   "source": [
    "# Object Tracking Using SIFT\n",
    "\n",
    "This notebook is tied to the previous notebook where I recreated the SIFT detector. Now I'll use the SIFT detector to see if I can track objects or patterns within an image. I'll be using the same traffic video that I did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30a8cd4d-c4db-42f1-936b-4358593d2e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Video\n",
    "from ipywidgets import Output\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import cycle\n",
    "from operator import itemgetter\n",
    "from uuid import UUID, uuid1 as uuid\n",
    "from dataclasses import dataclass, field\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "matplotlib.rcdefaults()\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = (16, 12)\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "intpl = lambda t: tuple(map(int, t))\n",
    "take = lambda n, it: map(itemgetter(1), zip(range(n), it))\n",
    "\n",
    "video_file = '../../data/traffic-video.mp4'\n",
    "intermediate_file_template = '../../data/traffic-video-{step}.mp4'\n",
    "\n",
    "Video(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553d33f-8ddb-4e12-b008-51b050700596",
   "metadata": {},
   "source": [
    "Let's first see if we can track the grayscale version of the video file. So we'll convert the video to grayscale first. We need to grab the frame size and framerate from our previous capture. Since we'll be writing a lot of video, I'll create some functions to handle most of the common routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09e3563-e0d1-41dc-9277-75f530be83db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ded8afcf9d645f6bb02f5bb01235a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Frames:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-grayscale.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@contextmanager\n",
    "def video_capture(video_file):\n",
    "    # Create capture and get params\n",
    "    capture = cv2.VideoCapture(video_file)\n",
    "    yield capture\n",
    "    capture.release()\n",
    "    \n",
    "    \n",
    "@contextmanager\n",
    "def video_writer(video_file, size, framerate, fourcc='avc1', isColor=True):\n",
    "    # Create writer with params\n",
    "    fourcc = cv2.VideoWriter_fourcc(*fourcc)\n",
    "    writer = cv2.VideoWriter(video_file, fourcc, framerate, size, isColor=isColor)\n",
    "    yield writer\n",
    "    writer.release()\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def video_pipe(from_file, to_file, fourcc='avc1', isColor=True):\n",
    "    with video_capture(from_file) as cap:\n",
    "        # Get capture params\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        framerate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        with video_writer(to_file, (width, height), framerate, fourcc, isColor) as wrt:\n",
    "            yield (cap, wrt)\n",
    "\n",
    "    \n",
    "def frame_iter(capture, desc='Processing Frames'):\n",
    "    frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    progress = tqdm(desc=desc, total=frames)\n",
    "    has_frames, frame = capture.read()\n",
    "    while has_frames:\n",
    "        yield frame\n",
    "        progress.update()\n",
    "        has_frames, frame = capture.read()\n",
    "    \n",
    "\n",
    "grayscale_file = intermediate_file_template.format(step='grayscale')\n",
    "\n",
    "# Convert frames to grayscale\n",
    "with video_pipe(video_file, grayscale_file, isColor=False) as (cap, writer):\n",
    "    for frame in frame_iter(cap):\n",
    "        grayscale = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        writer.write(grayscale)\n",
    "\n",
    "# Display new Video\n",
    "Video(grayscale_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33639ab7-9d45-4306-acb0-5eb995370f86",
   "metadata": {},
   "source": [
    "Before we apply OpenCV's SIFT detector to each frame, we need to mask out the parts of the image we want to focus on, i.e. the cars driving on the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6543c52e-5b6c-42a3-b948-0ea3c6881d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e6f49b18d44cc4a7bdff180b40d90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Frames:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-masked.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Road line boundary\n",
    "r_x1 = 860\n",
    "r_x2 = 150\n",
    "\n",
    "masked_file = intermediate_file_template.format(step='masked')\n",
    "\n",
    "# Get width and height of image\n",
    "cap = cv2.VideoCapture(grayscale_file)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "cap.release()\n",
    "\n",
    "# Image pixel space\n",
    "x,y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "\n",
    "# Mask\n",
    "road_mask = x + y * (r_x1 - r_x2) / h - r_x1 > 0\n",
    "chrmask = ((-x + 665 > 0) \\\n",
    "        | (-y + 640 > 0) \\\n",
    "        | (x - 1155 > 0) \\\n",
    "        | (y - 690 > 0))\n",
    "track_mask = chrmask & road_mask\n",
    "track_mask = track_mask.astype(np.uint8)\n",
    "\n",
    "# Create framed video\n",
    "with video_pipe(grayscale_file, masked_file, isColor=False) as (cap, writer):\n",
    "    for frame in frame_iter(cap):\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        masked_frame = np.where(track_mask, gray_frame, 0)\n",
    "        writer.write(masked_frame)\n",
    "        \n",
    "# Display new Video\n",
    "Video(masked_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71e907-8425-4952-b6a8-6fe5eb2b8fdf",
   "metadata": {},
   "source": [
    "I'm going to draw the points manually as I want to implement my own tracking but still display them. So I created my own colormap to distinguish the different keypoints. All colormap points are displayed neatly below in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b1cbac9-1750-407b-9c4e-0de9ec83a257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x3600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create colormap\n",
    "n_colors = 360\n",
    "q = n_colors // 6\n",
    "p = np.linspace(0, 1, q)\n",
    "w = np.zeros(n_colors + 2*q)\n",
    "w[2*q:3*q] = p\n",
    "w[3*q:5*q] = 1.0\n",
    "w[5*q:6*q] = 1.0 - p\n",
    "colormap = 255 * np.array([(1.0 - w[q:-q]), w[:-2*q], w[2*q:]]).T\n",
    "np.random.shuffle(colormap)\n",
    "\n",
    "# Display colormap as image\n",
    "imheight = 10\n",
    "imscale = 1\n",
    "imwidth = n_colors // imheight\n",
    "x, y = np.meshgrid(np.arange(imwidth), np.arange(imheight))\n",
    "image = colormap[(y*imwidth + x),:].astype(np.uint8)\n",
    "rc_params = {\n",
    "    'figure.figsize': (imheight*imscale, imwidth*imscale),\n",
    "    'xtick.bottom': False,\n",
    "    'xtick.labelbottom': False,\n",
    "    'ytick.left': False,\n",
    "    'ytick.labelleft': False\n",
    "}\n",
    "with plt.rc_context(rc_params):\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Convert colormap to list of tuples\n",
    "colormap = list(map(tuple, colormap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8c6b2-aee7-4dec-acb1-8b76a7eaaf10",
   "metadata": {},
   "source": [
    "Now we'll run SIFT detection to find all keypoints and descriptors in the frame.\n",
    "\n",
    "Since I'm working on my own tracking algorithm, I'll use my own keypoint drawing system using the custom colormap above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45ffb1b6-672b-4bc2-943a-e751740113c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f0595f526248cab268b3b44cf5649d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Keypoint Data:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f37d1c779434d2588613d13807493b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing Keypoints:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-keypoints.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_file = intermediate_file_template.format(step='keypoints')\n",
    "\n",
    "def draw_keypoints(img, keypoints, colors=cycle(colormap)):\n",
    "    kpimg = img.copy()\n",
    "    for kp, color in zip(keypoints, colors):\n",
    "        r = int(kp.size / 2)\n",
    "        c = np.array(kp.pt, dtype=np.int32)\n",
    "        v = np.array([ np.cos(kp.angle), np.sin(kp.angle) ])\n",
    "        u = (c + r*v).astype(np.uint32)\n",
    "        cv2.circle(kpimg, c, r, color, 1)\n",
    "        cv2.arrowedLine(kpimg, c, u, color, 1)\n",
    "    return kpimg\n",
    "        \n",
    "sift = cv2.SIFT.create()\n",
    "keypoint_data = []\n",
    "\n",
    "with video_capture(grayscale_file) as cap:\n",
    "    for frame in frame_iter(cap, desc='Generating Keypoint Data'):\n",
    "        keypoints, descriptors = sift.detectAndCompute(frame, track_mask)\n",
    "        keypoint_data.append((keypoints, descriptors))\n",
    "\n",
    "with video_pipe(grayscale_file, keypoints_file) as (cap, wrt):\n",
    "    for (keypoints,_), frame in zip(keypoint_data, frame_iter(cap, desc=\"Drawing Keypoints\")):\n",
    "        keypoints_frame = draw_keypoints(frame, keypoints)\n",
    "        wrt.write(keypoints_frame)\n",
    "        \n",
    "Video(keypoints_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc48437-9467-4409-ac99-4550ee0f43c4",
   "metadata": {},
   "source": [
    "If we look at these keypoints, we notice they're jittering a lot. That's because each frame is treated independently, and each detection is treated as a new keypoint. We need to correlate each keypoint in a frame with the keypoints in the frame before using a matching algorithm.\n",
    "\n",
    "We'll use the euclidean distance to match the descriptors of keypoints with each other. For each pair of adjacent frames, we take both sets of descriptors, and attempt to reorder them based on which keypoints match which with the previous frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2cdf7789-318c-4b3a-9973-a795567894ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5160f32fdf2441378a8e2492c2590614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reordering keypoints:   0%|          | 0/749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9e93861ee642f8be2d1091a21e697a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing Keypoints:   0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../../data/traffic-video-tracked.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracked_file = intermediate_file_template.format(step='tracked')\n",
    "match_threshold = 100.0\n",
    "\n",
    "def reorder_descriptors(dsc_a, dsc_b):\n",
    "    \"\"\"\n",
    "    Match descriptors in dsc_a with those in\n",
    "    dsc_b and find required indeces to reorder\n",
    "    dsc_b to best match dsc_a\n",
    "    \"\"\"\n",
    "    dsc_a = dsc_a[:,np.newaxis,:]\n",
    "    dsc_b = dsc_b[np.newaxis,:,:]\n",
    "    distance = np.sum((dsc_a - dsc_b)**2, axis=2)\n",
    "    p_idx = np.arange(distance.shape[0])\n",
    "    m_idx = np.argmin(distance, axis=1)\n",
    "    return np.where(\n",
    "        distance.min(axis=1) < match_threshold,\n",
    "        m_idx,\n",
    "        p_idx)\n",
    "\n",
    "\n",
    "max_kps = max( len(kp) for kp,_ in keypoint_data )\n",
    "colors = list(take(max_kps, cycle(colormap)))\n",
    "\n",
    "# Get max number of keypoints in a given frame\n",
    "ro_keypoint_data = [ keypoint_data[0] ]\n",
    "\n",
    "for (kp_list, dsc) in tqdm(keypoint_data[1:], desc='Reordering keypoints'):\n",
    "    kps = np.array(kp_list)\n",
    "    last_dsc = ro_keypoint_data[-1][1]\n",
    "    ro_idx = reorder_descriptors(last_dsc, dsc)\n",
    "    ro_kps = kps[ro_idx]\n",
    "    ro_dsc = dsc[ro_idx]\n",
    "    ro_keypoint_data.append((list(ro_kps), ro_dsc))\n",
    "    \n",
    "with video_pipe(grayscale_file, tracked_file) as (cap, wrt):\n",
    "    for (keypoints,_), frame in zip(ro_keypoint_data, frame_iter(cap, desc=\"Drawing Keypoints\")):\n",
    "        keypoints_frame = draw_keypoints(frame, keypoints, colors=colors)\n",
    "        wrt.write(keypoints_frame)\n",
    "        \n",
    "Video(tracked_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
